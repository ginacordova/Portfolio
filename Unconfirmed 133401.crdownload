
# coding: utf-8

# # Web Scraping Project

# In[4]:


# The imports
from bs4 import BeautifulSoup
import requests


# In[3]:


import pandas as pd
from pandas import Series, DataFrame


# In[36]:


# Set an object called url with the data source website address
url='https://en.wikipedia.org/wiki/Gross_world_product'


# In[37]:


# Set an object called result 
# Use requests.get method on the url object
# uncomment the below code (giving errors)
result=requests.get(url) 


# In[ ]:


#Set an object called c with the content of that result
c=result.content


# In[ ]:


# Make an object using BeautifulSoup
soup = BeautifulSoup(c)


# In[38]:


# Make an object called summary
# Call the find method on the soup object
# Pass a dictionary for the class and id (html elements for the table)
summary = soup.find('div', {'class':'list-land', 'id':'content'})

# 
tables = summary.find_all('table')


# In[39]:


# Setup an empty data list
data = []

# Set 'rows' as the first index object for every findAll() table row
rows = tables[0].findAll('tr')

# For every table row in 'rows' have the 'cols' object find everything with an html td tag
for tr in rows:
    cols = tr.findAll('td')
# Check to see if text is in the row     
    for td in cols:
        text = td.find(text=True)
        print text,
# Append object 'text' to data list        
        data.append(text)


# In[40]:


data


# In[41]:


# Use a for loop to go through the list and grab only the cells with a pdf file in them

reports = []
date = []

# Counting object
index= 0

for item in data:
    if 'pdf' in item:
        date.append(data[index-1])
# Clean up data        
        reports.append(item.replace(u'\xa0', u' '))
        
    index += 1    


# In[42]:


# Concatenate Series into a dataframe
date = Series (date)
reports = Series(reports)


# In[44]:


# Concatenate into a dataframe
legislative_df = pd.concat([date, reports], axis=1)


# In[45]:


# Setup the columns
legislative_df.columns = ['Date', 'Report']


# In[47]:


# Show the new dataframe
legislative_df


# ## Thank you!
